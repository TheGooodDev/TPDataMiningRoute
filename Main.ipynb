{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7451b8e8",
   "metadata": {},
   "source": [
    "# Le notebook que yohan n'a pas fait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8b8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import os\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.cm import bwr as cmap\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9722cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristiques = pd.read_csv('./Dataset/all_years/caracteristiques-2019-22.csv')\n",
    "departements = pd.read_csv('./Dataset/departements-france.csv')\n",
    "lieux = pd.read_csv('./Dataset/all_years/lieux-2019-22.csv')\n",
    "usager = pd.read_csv('./Dataset/all_years/usagers-2019-22.csv')\n",
    "vehicule = pd.read_csv('./Dataset/all_years/vehicules-2019-22.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b6555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vehicle_count(caracteristiques, vehicules):\n",
    "    # Compter le nombre de véhicules par \"Num_Acc\" dans le DataFrame vehicules\n",
    "    vehicules_count = vehicules.groupby(\"Accident_Id\")[\"id_vehicule\"].count().reset_index().rename(columns={'id_vehicule':'Nombre_vehicule'})\n",
    "    \n",
    "    # Fusionner le DataFrame caracteristiques avec le DataFrame vehicules_count en utilisant la colonne \"Num_Acc\"\n",
    "    merged_df = caracteristiques.merge(vehicules_count, on='Accident_Id')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def merge_user_count(caracteristiques,usagers):\n",
    "    # Compter le nombre d'usagers par \"Accident_Id\" et par sexe dans le DataFrame usagers\n",
    "    usagers_count = usagers.groupby([\"Accident_Id\", \"sexe\"])[\"id_vehicule\"].count().unstack(fill_value=0)\n",
    "    usagers_count.rename(columns={1: 'Nombre_Hommes', 2: 'Nombre_Femmes'}, inplace=True)\n",
    "    \n",
    "    # Fusionner le DataFrame caracteristiques avec usagers_count en utilisant la colonne \"Num_Acc\"\n",
    "    merged_df = caracteristiques.merge(usagers_count, on='Accident_Id')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def replace_with_mapper(df, col, mapper):\n",
    "    df[col] = df[col].replace(mapper)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Remplacement des heures par le moment de la journée\n",
    "matinee = ['06:','07:','08:','09:','10:']\n",
    "journee = ['11:','12:','13:','14:','15:','16:','17:']\n",
    "soiree = ['18:','19:','20:','21:','22:','23:']\n",
    "nuit = ['00:','01:','02:','03:','04:','05:']\n",
    "\n",
    "def replace_hour(hour):\n",
    "    hour = hour[:3]\n",
    "    if hour in matinee:\n",
    "        return 'Matinée'\n",
    "    elif hour in journee:\n",
    "        return 'Journée'\n",
    "    elif hour in soiree:\n",
    "        return 'Soirée'\n",
    "    elif hour in nuit:\n",
    "        return 'Nuit'\n",
    "    else:\n",
    "        'Lucas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ff71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['19', '20', '21', '22']\n",
    "files_name = [\"caracteristiques-20\", \"lieux-20\", \"usagers-20\", \"vehicules-20\"]\n",
    "if not os.path.exists('./Dataset/all_years'):\n",
    "    os.mkdir('./Dataset/all_years/', 0o666) \n",
    "for file_name in files_name:\n",
    "    result = []\n",
    "    for year in years:\n",
    "        file_path = f'./Dataset/{year}/{file_name}{year}.csv'\n",
    "        sep = ';'\n",
    "        df = pd.read_csv(file_path, sep=sep, low_memory=False)\n",
    "        # if 'id_usager' in df.columns:\n",
    "        #     df = df.drop(columns=['id_usager'])\n",
    "        result.append(df)\n",
    "    combined_df = pd.concat(result, ignore_index=True)\n",
    "    combined_df.to_csv(f'./Dataset/all_years/{file_name}{years[0]}-{years[-1]}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1055671",
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteristiques = caracteristiques.drop(['com', 'adr', 'lat', 'long'], axis=1)\n",
    "\n",
    "# Remplacer les numéros des mois par leurs noms\n",
    "caracteristiques.mois = caracteristiques.mois.map({\n",
    "    1:'Janvier',\n",
    "    2:'Février',\n",
    "    3:'Mars',\n",
    "    4:'Avril',\n",
    "    5:'Mai',\n",
    "    6:'Juin',\n",
    "    7:'Juillet',\n",
    "    8:'Août',\n",
    "    9:'Septembre',\n",
    "    10:'Octobre',\n",
    "    11:'Novembre',\n",
    "    12:'Décembre'\n",
    "}, na_action=None)\n",
    "\n",
    "caracteristiques.hrmn = caracteristiques.hrmn.apply(replace_hour)\n",
    "\n",
    "caracteristiques = replace_with_mapper(caracteristiques, 'lum', {\n",
    "    1 : 'Plein jour',\n",
    "    2 : 'Crépuscule ou aube',\n",
    "    3 : 'Nuit sans éclairage public',\n",
    "    4 : 'Nuit avec éclairage public non allumé',\n",
    "    5 : 'Nuit avec éclairage public allumé'\n",
    "})\n",
    "\n",
    "# Remplacer les numéros de départements par leur noms\n",
    "departements_mapper = departements.set_index('code_departement')['nom_departement'].to_dict()\n",
    "caracteristiques = replace_with_mapper(caracteristiques, 'dep', departements_mapper)\n",
    "\n",
    "caracteristiques = replace_with_mapper(caracteristiques, 'agg', {\n",
    "    1 : 'Hors agglomération',\n",
    "    2 : 'En agglomération'\n",
    "})\n",
    "caracteristiques = replace_with_mapper(caracteristiques, 'int', {\n",
    "    1 : 'Hors intersection',\n",
    "    2 : 'Intersection en X',\n",
    "    3 : 'Intersection en T',\n",
    "    4 : 'Intersection en Y',\n",
    "    5 : 'Intersection à plus de 4 branches',\n",
    "    6 : 'Giratoire',\n",
    "    7 : 'Place',\n",
    "    8 : 'Passage à niveau',\n",
    "    9 : 'Autre intersection'\n",
    "})\n",
    "caracteristiques = replace_with_mapper(caracteristiques, 'atm', {\n",
    "    -1 :'Non renseigné',\n",
    "    1 : 'Normale',\n",
    "    2 : 'Pluie légère',\n",
    "    3 : 'Pluie forte',\n",
    "    4 : 'Neige - grêle',\n",
    "    5 : 'Brouillard - fumée',\n",
    "    6 : 'Vent fort - tempête',\n",
    "    7 : 'Temps éblouissant',\n",
    "    8 : 'Temps couvert',\n",
    "    9 : 'Autre'\n",
    "})\n",
    "caracteristiques = replace_with_mapper(caracteristiques, 'col', {\n",
    "    -1 :'Non renseigné',\n",
    "    1 : 'Deux véhicules - frontale',\n",
    "    2 : 'Deux véhicules – par l’arrière',\n",
    "    3 : 'Deux véhicules – par le coté',\n",
    "    4 : 'Trois véhicules et plus – en chaîne',\n",
    "    5 : 'Trois véhicules et plus - collisions multiples',\n",
    "    6 : 'Autre collision',\n",
    "    7 : 'Sans collision'\n",
    "})\n",
    "\n",
    "caracteristiques = caracteristiques.merge(\n",
    "replace_with_mapper(lieux.loc[:, ['Accident_Id', 'catr']], 'catr', {\n",
    "    1:'Autoroute',\n",
    "    2:'Route nationale',\n",
    "    3:'Route Départementale',\n",
    "    4:'Voie Communales',\n",
    "    5:'Hors réseau public',\n",
    "    6:'Parc de stationnementouvert à la circulation publique',\n",
    "    7:'Routes de métropole urbaine',\n",
    "    9:'autre'\n",
    "}), on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques = caracteristiques.merge(\n",
    "replace_with_mapper(lieux.loc[:, ['Accident_Id', 'circ']], 'circ', {\n",
    "    -1:'Non renseigné',\n",
    "    1:'A sens unique',\n",
    "    2:'Bidirectionnelle',\n",
    "    3:'A chaussées séparées',\n",
    "    4:'Avec voies d’affectation variable'\n",
    "}), on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques = caracteristiques.merge(lieux.loc[:, ['Accident_Id', 'nbv']], on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques = caracteristiques.merge(\n",
    "replace_with_mapper(lieux.loc[:, ['Accident_Id', 'surf']], 'surf', {\n",
    "    -1:'Non renseigné',\n",
    "    1:'Normale',\n",
    "    2:'Mouillée',\n",
    "    3:'Flaques',\n",
    "    4:'Inondée',\n",
    "    5:'Enneigée',\n",
    "    6:'Boue',\n",
    "    7:'Verglacée',\n",
    "    8:'Corps gras – huile',\n",
    "    9:'Autre'\n",
    "}), on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques = caracteristiques.merge(\n",
    "replace_with_mapper(lieux.loc[:, ['Accident_Id', 'infra']], 'infra', {\n",
    "    -1:'Non renseigné',\n",
    "    0:'Aucun',\n",
    "    1:'Souterrain -tunnel',\n",
    "    2:'Pont -autopont',\n",
    "    3:'Bretelle d’échangeur ou de raccordement',\n",
    "    4:'Voie ferrée',\n",
    "    5:'Carrefour aménagé',\n",
    "    6:'Zone piétonne',\n",
    "    7:'Zone de péage',\n",
    "    8:'Chantier',\n",
    "    9:'Autres'\n",
    "}), on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques = caracteristiques.merge(\n",
    "replace_with_mapper(lieux.loc[:, ['Accident_Id', 'situ']], 'situ', {\n",
    "    -1:'Non renseigné',\n",
    "    0:'Aucun',\n",
    "    1:'Sur chaussée',\n",
    "    2:'Sur bande d’arrêt d’urgence',\n",
    "    3:'Sur accotement',\n",
    "    4:'Sur trottoir',\n",
    "    5:'Sur piste cyclable',\n",
    "    6:'Sur autre voie spéciale',\n",
    "    8:'Autres'\n",
    "}), on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques = caracteristiques.merge(lieux.loc[:, ['Accident_Id', 'vma']], on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques.rename(columns={\n",
    "    'an': 'année',\n",
    "\t'hrmn': 'horraire',\n",
    "    'lum': 'luminosité',\n",
    "    'dep': 'département',\n",
    "    'agg': 'agglomération',\n",
    "    'int': 'intersection',\n",
    "    'atm': 'atmosphere',\n",
    "    'col': 'collision',\n",
    "    'catr': 'catégorie de route',\n",
    "    'circ': 'régime de circulation',\n",
    "    'nbv': 'nombre de voix',\n",
    "    'surf': 'etat de la surface',\n",
    "    'infra': 'infrastructure',\n",
    "    'situ': 'situation',\n",
    "    'vma': 'vitesse max autorisée'\n",
    "}, inplace=True)\n",
    "\n",
    "caracteristiques.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4f320",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vehicule['catv'] = vehicule['catv'].replace([0, 99, -1],  ['indéterminable', 'autre véhicule', 'indéterminé'])\n",
    "vehicule['catv'] = vehicule['catv'].replace([10, 13, 14, 15, 16, 17, 20, 21, 37, 38, 39, 40], 'vehicules lourd')\n",
    "vehicule['catv'] = vehicule['catv'].replace([3, 7, 35, 36, 50, 60], 'vehicules leger')\n",
    "vehicule['catv'] = vehicule['catv'].replace([1, 2, 30, 31, 32, 33, 34, 41, 42, 43, 80], 'vehicules deux roues')\n",
    "\n",
    "r = vehicule.groupby('Accident_Id')['catv'].value_counts().unstack().fillna(0)\n",
    "r.reset_index(inplace=True)\n",
    "r[['indéterminable', 'autre véhicule', 'indéterminé', 'vehicules lourd', 'vehicules leger', 'vehicules deux roues']] = r[['indéterminable', 'autre véhicule', 'indéterminé', 'vehicules lourd', 'vehicules leger', 'vehicules deux roues']].astype(int)\n",
    "caracteristiques = caracteristiques.merge(r, on='Accident_Id', how='left')\n",
    "\n",
    "caracteristiques.head(5)\n",
    "\n",
    "\n",
    "r = usager.groupby('Accident_Id')['grav'].value_counts().unstack().fillna(0)\n",
    "r.reset_index(inplace=True)\n",
    "r.rename(columns={\n",
    "    -1:'gravité indéterminé',\n",
    "    1:'indemne',\n",
    "    2:'tué',\n",
    "    3:'blessé hospitalisé',\n",
    "    4:'blessé léger'\n",
    "}, inplace=True)\n",
    "r[['gravité indéterminé', 'indemne', 'tué', 'blessé hospitalisé', 'blessé léger']] = r[['gravité indéterminé', 'indemne', 'tué', 'blessé hospitalisé', 'blessé léger']].astype(int)\n",
    "\n",
    "caracteristiques = caracteristiques.merge(r, on='Accident_Id', how='left')\n",
    "\n",
    "r = usager.groupby('Accident_Id')['sexe'].value_counts().unstack().fillna(0)\n",
    "r.reset_index(inplace=True)\n",
    "r.rename(columns={\n",
    "    -1:'autre',\n",
    "    1:'hommes',\n",
    "    2:'femmes'\n",
    "}, inplace=True)\n",
    "r[['autre', 'hommes', 'femmes']] = r[['autre', 'hommes', 'femmes']].astype(int)\n",
    "\n",
    "caracteristiques = caracteristiques.merge(r, on='Accident_Id', how='left')\n",
    "\n",
    "usager['année'] = usager['Accident_Id'].map(caracteristiques.set_index('Accident_Id')['année'])\n",
    "usager['age_category'] = pd.cut(\n",
    "    usager['année'] - usager['an_nais'],\n",
    "    bins=[0, 18, 59, float('inf')],\n",
    "    labels=[\"jeune\", \"adulte\", \"senior\"],\n",
    "    right=True\n",
    ")\n",
    "\n",
    "number_of_people = usager.groupby(by='Accident_Id')['age_category'].value_counts().unstack().fillna(0)\n",
    "number_of_people.reset_index(inplace=True)\n",
    "\n",
    "caracteristiques = caracteristiques.merge(number_of_people, on='Accident_Id', how='right')\n",
    "\n",
    "# Clean\n",
    "usager.drop(columns=['année'])\n",
    "usager.drop(columns=['age_category'])\n",
    "\n",
    "r = usager.groupby('Accident_Id')['catu'].value_counts().unstack().fillna(0)\n",
    "r.reset_index(inplace=True)\n",
    "r.rename(columns={\n",
    "    1:'conducteur',\n",
    "    2:'passager',\n",
    "    3:'piéton'\n",
    "}, inplace=True)\n",
    "r[['conducteur', 'passager', 'piéton']] = r[['conducteur', 'passager', 'piéton']].astype(int)\n",
    "caracteristiques = caracteristiques.merge(r, on='Accident_Id', how='left')\n",
    "\n",
    "display(caracteristiques.info())\n",
    "caracteristiques.sort_values(by=\"département\").head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_name = caracteristiques.iloc[:,17:].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b94e1f5",
   "metadata": {},
   "source": [
    "## Normalisation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3337ab",
   "metadata": {},
   "source": [
    "## Analyse en composantes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_top_comp(components, feature_names, n_top_comp):\n",
    "    seuil=1/np.sqrt(len(feature_names))-0.02\n",
    "    for comp_idx, topic in enumerate(components):\n",
    "        feature_pos=feature_names[topic>=seuil]\n",
    "        feature_neg=feature_names[topic<-seuil]\n",
    "        n_top_comp_pos=min(n_top_comp,feature_pos.shape[0])\n",
    "        n_top_comp_neg=min(n_top_comp,feature_neg.shape[0])\n",
    "        feature_pos=feature_names[topic.argsort()[::-1][:n_top_comp_pos]]\n",
    "        feature_neg=feature_names[topic.argsort()[:n_top_comp_neg]]\n",
    "        fea=np.concatenate((feature_pos,feature_neg))\n",
    "        com_pos=np.sort(topic)[::-1][:n_top_comp_pos]\n",
    "        com_neg=np.sort(topic)[:n_top_comp_neg]\n",
    "        com=np.concatenate((com_pos,com_neg))\n",
    "                \n",
    "        c_normal = colors.PowerNorm(1,vmin=min(com),vmax=max(com))\n",
    "        _COLORS = cmap(c_normal(com))\n",
    "                       \n",
    "        components = pd.DataFrame(np.array(com),index =fea) \n",
    "        fig, ax = plt.subplots(figsize = (5,3))\n",
    "        # Plot the feature weights as a function of the components\n",
    "        components.plot(ax = ax, kind = 'bar',align = \"center\", color=_COLORS)\n",
    "        #plt.xlabel(\"Dimension #%d \" % comp_idx)\n",
    "        ax.set_ylabel(\"Feature Weights\") \n",
    "        red_patch = mpatches.Patch(color='red', label=\"Composante-%d \" % (comp_idx+1))\n",
    "        ax.legend(handles=[red_patch])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import colors as clr\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import operator\n",
    "\n",
    "def visualize_data(name):\n",
    "    columns_name = caracteristiques.iloc[:,17:].columns\n",
    "    newDfCarac = caracteristiques.groupby([name])[columns_name].mean()\n",
    "    Xcarac=newDfCarac.iloc[:,0:].values\n",
    "    label_carac=newDfCarac.index\n",
    "    SS=StandardScaler()\n",
    "    SS.fit(Xcarac)\n",
    "    Xcarac_ss=SS.transform(Xcarac)\n",
    "    pca=PCA(n_components=2)\n",
    "    pca.fit(Xcarac_ss)\n",
    "    Xcarac_pca=pca.transform(Xcarac_ss)\n",
    "    print_top_comp(pca.components_[:2,:], newDfCarac.columns, len(newDfCarac.columns))\n",
    "    coef = np.transpose(pca.components_)\n",
    "    cols = ['y'+str(x+1) for x in range(2)]\n",
    "    pc_infos = pd.DataFrame(coef, columns=cols, index=newDfCarac.iloc[:,0:].columns)\n",
    "    plt.Circle((0,0),radius=10, color='g', fill=False)\n",
    "    circle1=plt.Circle((0,0),radius=1, color='g', fill=False)\n",
    "    # Ajouter les axes et donner la limite pour chaque axe\n",
    "    fig, axes= plt.subplots(figsize=(6,6))\n",
    "    axes.set_xlim(-1,1)\n",
    "    axes.set_ylim(-1,1)\n",
    "    fig.gca().add_artist(circle1)\n",
    "    plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\n",
    "    plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\n",
    "    axes.add_artist(circle1)\n",
    "    \n",
    "    # Affichage de chaque variable explicative dans la cercle\n",
    "    for idx in range(len(pc_infos[\"y1\"])):\n",
    "        x = pc_infos[\"y1\"][idx]\n",
    "        y = pc_infos[\"y2\"][idx]\n",
    "        plt.plot([0.0,x],[0.0,y],'k-')\n",
    "        plt.plot(x, y, 'rx')\n",
    "        plt.annotate(pc_infos.index[idx], xy=(x,y))\n",
    "    plt.xlim((-1,1))\n",
    "    plt.ylim((-1,1))\n",
    "    plt.title(\"Circle of Correlations\")\n",
    "    plt.show()\n",
    "    \n",
    "    plt.scatter(Xcarac_pca[:, 0], Xcarac_pca[:, 1])\n",
    "    for label, x, y in zip(label_carac, Xcarac_pca[:, 0], Xcarac_pca[:, 1]):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(-0.2, 0.2), textcoords='offset points')\n",
    "    plt.show()\n",
    "    \n",
    "    testArr = ['complete','single','ward','average']\n",
    "    for i in range(len(testArr)) :\n",
    "        Z = linkage(Xcarac_ss, testArr[i])\n",
    "        plt.figure(figsize=(25,10))\n",
    "        plt.title('Hierarchical Clustering Dendrogram with ' + testArr[i])\n",
    "        plt.xlabel('sample index')\n",
    "        plt.ylabel('distance')\n",
    "        dendrogram(\n",
    "            Z,\n",
    "            leaf_rotation=90,    # rotates the x axis labels\n",
    "            leaf_font_size=8.,   # font size for the x axis labels\n",
    "        )\n",
    "        plt.show()\n",
    "    \n",
    "    bestScore = { }\n",
    "    \n",
    "    for i in np.arange(2,len(label_carac)):\n",
    "        km = KMeans(n_clusters=i, random_state=1)\n",
    "        km.fit(Xcarac_ss)\n",
    "        clustering_kmeans = km.labels_\n",
    "        score_s = silhouette_score(Xcarac_ss, clustering_kmeans, metric='euclidean')\n",
    "        print('Silhouette score pour {0:d} clusters vaut {1:.3f}'.format(i,score_s))\n",
    "        bestScore[i] = score_s\n",
    "    \n",
    "    number_cluster = max(bestScore.items(), key=operator.itemgetter(1))\n",
    "    print('Le meilleur nombre de cluster est {0:d}'.format(number_cluster[0]))\n",
    "    \n",
    "    km = KMeans(n_clusters=number_cluster[0], random_state=1)\n",
    "    km.fit(Xcarac_ss)\n",
    "    clustering_kmeans=km.labels_\n",
    "    colors = ['red','yellow','blue','pink','black']\n",
    "    plt.scatter(Xcarac_pca[:,0], Xcarac_pca[:,1], c= clustering_kmeans, cmap=clr.ListedColormap(colors))\n",
    "    for label, x, y in zip(label_carac, Xcarac_pca[:,0], Xcarac_pca[:,1]):\n",
    "        plt.annotate(label,xy=(x,y), xytext=(-0.2,0.2), textcoords='offset points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d5210d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_data(\"département\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988d567",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_data(\"horraire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a2731",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize_data(\"mois\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f56968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907ae20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f1fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
